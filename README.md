# Scraping with Scrapy

## Description

The purpose of this component is to scrape contact information from a list of urls. I got part of the way there, but have sinced realized that search engine APIs will be more fruitful. I'll keep this project around as it is better if we need to scale up or gather more in depth information from the sites in question. In general, it will offer far more control. 

## Installation

The project uses the most recent version of the python. You'll need to import scrapy, threading, etree, and lxml if you want to run the project but python will guide you through the process. 

## Usage

Look at the batch_scraper folder. The batch_scraper contains a 'scrape_urls' function that takes a list of urls and returns a list of dictionaries with the contact information. It is the main interface for the project. 

The testing is done with 'run and debug' in vscode. Look at the testing folder for more information.

## Architecture

### Scrapy Scraping

This project is built using scrapy. The best documentation for scrapy is the [official documentation](https://docs.scrapy.org/en/latest/) and the [scrapy playbook](https://thepythonscrapyplaybook.com/). I think that the best way to understand the library at a glance is to look at this diagram:

![Scrapy Archetecture](https://docs.scrapy.org/en/latest/_images/scrapy_architecture_02.png)

Once you get a sense of the architecture, the code becomes much easier to understand. This component modifies is the spider, items, and pipelines. Right now, all the spider does, is pass a phone number that it finds site footer into the pipeline. Thus, the item only has a phone number in it. The pipeline is a little more complex. 

Pipelines in scrapy are designed to asynchronously place items into a database or file. However, this would complicate the implementation. Instead, I pass a python dictionary to the spider, and the pipeline feeds the item into the dictionary. This is far from optimal, but it things simple. 

The spider is in its own folder because scrapy is designed such that you can have multiple spiders in a single project. This is useful if you want to scrape one category of site with one set of rules and another category of site with another set of rules. However, I only have one spider, so the folder is a little redundant.

Although the spider itself only pulls phone numbers, it contains the peices for a more powerful implementation. The first tool is the 'html_to_struct'. This uses the lxml library to convert the html into a tree structure. You can then you graph traversal algorithms to look through the html. In theory you could pull all text blocks or just look for the remaining contact info. The other tool is the 'get_links' funciton. This uses xpath to pull all links from the page with certain contitions. I was planning on 
using it to get the contact and about pages for each of the sites. 

Of course, both of these functions need refinement, but between the two of them, you should be able to get all the information that you need from these social services. The downside being that neither is very efficient, but that has never really been a priority. 

### Batch Scraper

As I mentioned earlier, scrapy is designed to be asynchronous. This is great for speed, but isn't ideal at a small scale as there is no one interface. The batch scraper is designed to be that interface. It takes a list of urls and returns a list of dictionaries with the contact information.

It uses the countdown latch structure acts as a modified semaphore that locks the main thread until all the spiders have indicated that they are done. 

### Testing

The last part of the code. Afrikaan Sahara provided a list of urls as examples. I manually pulled theit phone numbers and you can run the black box testing configuration to test the project against the phone numbers. VS Code provides the tools to step through the code and see what is happening.

## Upgrading

A lot of getting this project off the ground was understanding the library, so take your time to look through the documentation before you start making changes. That said, there is a lot of room for improvement. I'll sort the improvements that I was planning by class.

Whole project: 
 - Although the black box (component) testing is nice. Would be good to have some unit tests. Pytest is the best library for this.
 - Storing the scraped data in the spider is a bad design. It would be better to have a callback function in the batch scraper or better yet look into [item exporters](https://docs.scrapy.org/en/latest/topics/exporters.html?highlight=exporters#built-in-item-exporters-reference). Would need to store them in a separate file, which would complicate testing.

Spider: 
 - The get_links function should probably use the tree generated by the html_to_struct function. Also double check what it is returning.
 - The regexes for the phone numbers should be tested (main reasons for unit testing really).
 - should mention that parse can 'yield' items as their found. As the algorithm gets more advanced, this will be important.

Items/Pipeline: 
 - The stored data is the biggest thing. See above.
 - The items should have more fields.

Batch Scraper: 
 - I feel like the handlers can be simpler. Look into the scrapy signals.
 - Needs to handle case where item is dropped and item has an error.

Countdown Latch: 
 - Needs a timeout feature. Can't guarantee that the spiders will return.

Testing: 
 - I really like the way that this works, but should probably have asserts. 
 - Should add emails and addresses to the test cases.

## Contributing

I, Will McCormick, created this component. Ping me on LinkedIn if you have questions.